# Database Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=rag_pipeline
POSTGRES_USER=rag_user
POSTGRES_PASSWORD=change_me_in_production

# LLM Configuration
LLM_PROVIDER=local  # Options: local, openai, anthropic, together
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
TOGETHER_API_KEY=your_together_api_key_here

# Local LLM Settings (for llama.cpp)
LOCAL_MODEL_PATH=/models/llama-3-8b-instruct.Q4_K_M.gguf
LOCAL_MODEL_N_CTX=4096
LOCAL_MODEL_N_GPU_LAYERS=1  # Metal acceleration on M4 Pro

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_RELOAD=true

# Pipeline Configuration
CONFIDENCE_THRESHOLD=0.7
SENSITIVE_TOPIC_THRESHOLD=0.8
TOP_K_RETRIEVAL=5
HYBRID_SEARCH_ALPHA=0.5  # 0=BM25 only, 1=semantic only, 0.5=balanced

# Monitoring
ENABLE_METRICS=true
METRICS_PORT=9090
LOG_LEVEL=INFO

# Feature Flags
ENABLE_AUTO_RESPONSE=true
ENABLE_CONTINUOUS_LEARNING=true
ENABLE_PII_DETECTION=true
